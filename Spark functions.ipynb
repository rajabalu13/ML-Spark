{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkFunctions\").master(\"local[4]\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_location = \"C:/Users/Dinesh_2/Desktop/Patient_Dashboard_Active.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df_vw = spark.read.format(file_type) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .option(\"encoding\", \"UTF-8\") \\\n",
    "  .load(file_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+--------------------+--------------------+\n",
      "|             Patient|       DOB|        SSN|             Address|        Chart Status|\n",
      "+--------------------+----------+-----------+--------------------+--------------------+\n",
      "|     AASGAARD, LINDA|05/14/1943|       null|1400 CIRCLE CITY ...|            Admitted|\n",
      "|              Shuman|      null|       null|                null|                null|\n",
      "|       ABARCA, RAMON|08/26/1942|551-66-3306|7120 CORBIN AVE R...|          Discharged|\n",
      "|      ABBAS, ALIDAEE|04/03/1952|       null|13881 DAWSON ST G...|            Admitted|\n",
      "|              MARKIE|      null|       null|                null|                null|\n",
      "|      ABBOTT, CONNIE|09/17/1961|       null|5400 STINE ROAD B...|          Discharged|\n",
      "|      ABBOTT, CONNIE|09/17/1961|       null|5400 STINE ROAD B...|          Discharged|\n",
      "|    ABDOLI, MOHAMMAD|08/25/1948|443-74-4056|466 FLAGSHIP RD N...|           Non-admit|\n",
      "|ABEL MEDRIGAL, DE...|12/13/1971|       null|6401 33RD ST RIVE...|          Discharged|\n",
      "| ABERNATHY, PATRICIA|02/27/1949|       null|1665 M. STREET FR...|          Discharged|\n",
      "|       ABITIA, ANGEL|04/16/1955|561-04-6190|605 W. BROADWAY G...|          Discharged|\n",
      "|   ABITIA, ANGEL (I)|04/16/1955|561-04-6190|605 W BROADWAY AV...|          Discharged|\n",
      "|         ABLE, SHARI|06/06/1939|  566488725|225 North Crescen...|          Discharged|\n",
      "|             GILMORE|      null|       null|                null|                null|\n",
      "|              Isbell|      null|       null|                null|                null|\n",
      "|         ABLE, SHARI|06/06/1939|  566488725|225 North Crescen...|            Admitted|\n",
      "|             GILMORE|      null|       null|                null|                null|\n",
      "|              Isbell|      null|       null|                null|                null|\n",
      "|    ABOUDI, SHERAFAT|06/19/1940|       null|1019 S Wooster St...|          Discharged|\n",
      "|Above & Beyond, F...|      null|       null|                null|Transition TO CCTALW|\n",
      "+--------------------+----------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df_vw.select('Patient','DOB','SSN','Address','Chart Status')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Patient: string (nullable = true)\n",
      " |-- DOB: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Chart Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+--------------------+--------------------+---------------+\n",
      "|summary|             Patient|           DOB|                 SSN|             Address|   Chart Status|\n",
      "+-------+--------------------+--------------+--------------------+--------------------+---------------+\n",
      "|  count|               13657|         11204|                5553|               11188|          11525|\n",
      "|   mean|                null|          null| 4.905207640543478E8|                null|           null|\n",
      "| stddev|                null|          null|1.6742654088538024E8|                null|           null|\n",
      "|    min|\"ABRAHAMIAN, KRIK...|    01/01/1916|          36-46-5011|\t7324 Canby Ave R...|ARIAS, GABRIELA|\n",
      "|    max|                x, x|Sanjari, Nadia|         XXX-XX-8917|undecided LOS ANG...|lacanilao, emma|\n",
      "+-------+--------------------+--------------+--------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "|             Patient|Date of Birth|        SSN|                ADDR|        Chart_Status|\n",
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "|     AASGAARD, LINDA|   05/14/1943|       null|1400 CIRCLE CITY ...|            Admitted|\n",
      "|              Shuman|         null|       null|                null|                null|\n",
      "|       ABARCA, RAMON|   08/26/1942|551-66-3306|7120 CORBIN AVE R...|          Discharged|\n",
      "|      ABBAS, ALIDAEE|   04/03/1952|       null|13881 DAWSON ST G...|            Admitted|\n",
      "|              MARKIE|         null|       null|                null|                null|\n",
      "|      ABBOTT, CONNIE|   09/17/1961|       null|5400 STINE ROAD B...|          Discharged|\n",
      "|      ABBOTT, CONNIE|   09/17/1961|       null|5400 STINE ROAD B...|          Discharged|\n",
      "|    ABDOLI, MOHAMMAD|   08/25/1948|443-74-4056|466 FLAGSHIP RD N...|           Non-admit|\n",
      "|ABEL MEDRIGAL, DE...|   12/13/1971|       null|6401 33RD ST RIVE...|          Discharged|\n",
      "| ABERNATHY, PATRICIA|   02/27/1949|       null|1665 M. STREET FR...|          Discharged|\n",
      "|       ABITIA, ANGEL|   04/16/1955|561-04-6190|605 W. BROADWAY G...|          Discharged|\n",
      "|   ABITIA, ANGEL (I)|   04/16/1955|561-04-6190|605 W BROADWAY AV...|          Discharged|\n",
      "|         ABLE, SHARI|   06/06/1939|  566488725|225 North Crescen...|          Discharged|\n",
      "|             GILMORE|         null|       null|                null|                null|\n",
      "|              Isbell|         null|       null|                null|                null|\n",
      "|         ABLE, SHARI|   06/06/1939|  566488725|225 North Crescen...|            Admitted|\n",
      "|             GILMORE|         null|       null|                null|                null|\n",
      "|              Isbell|         null|       null|                null|                null|\n",
      "|    ABOUDI, SHERAFAT|   06/19/1940|       null|1019 S Wooster St...|          Discharged|\n",
      "|Above & Beyond, F...|         null|       null|                null|Transition TO CCTALW|\n",
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('DOB','Date of Birth').withColumnRenamed('Address','ADDR').withColumnRenamed('Chart Status','Chart_Status')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "|             Patient|Date of Birth|        SSN|                ADDR|        Chart_Status|\n",
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "|     AASGAARD, LINDA|   05/14/1943|         NA|1400 CIRCLE CITY ...|            Admitted|\n",
      "|              Shuman|         null|         NA|                  NA|                null|\n",
      "|       ABARCA, RAMON|   08/26/1942|551-66-3306|7120 CORBIN AVE R...|          Discharged|\n",
      "|      ABBAS, ALIDAEE|   04/03/1952|         NA|13881 DAWSON ST G...|            Admitted|\n",
      "|              MARKIE|         null|         NA|                  NA|                null|\n",
      "|      ABBOTT, CONNIE|   09/17/1961|         NA|5400 STINE ROAD B...|          Discharged|\n",
      "|      ABBOTT, CONNIE|   09/17/1961|         NA|5400 STINE ROAD B...|          Discharged|\n",
      "|    ABDOLI, MOHAMMAD|   08/25/1948|443-74-4056|466 FLAGSHIP RD N...|           Non-admit|\n",
      "|ABEL MEDRIGAL, DE...|   12/13/1971|         NA|6401 33RD ST RIVE...|          Discharged|\n",
      "| ABERNATHY, PATRICIA|   02/27/1949|         NA|1665 M. STREET FR...|          Discharged|\n",
      "|       ABITIA, ANGEL|   04/16/1955|561-04-6190|605 W. BROADWAY G...|          Discharged|\n",
      "|   ABITIA, ANGEL (I)|   04/16/1955|561-04-6190|605 W BROADWAY AV...|          Discharged|\n",
      "|         ABLE, SHARI|   06/06/1939|  566488725|225 North Crescen...|          Discharged|\n",
      "|             GILMORE|         null|         NA|                  NA|                null|\n",
      "|              Isbell|         null|         NA|                  NA|                null|\n",
      "|         ABLE, SHARI|   06/06/1939|  566488725|225 North Crescen...|            Admitted|\n",
      "|             GILMORE|         null|         NA|                  NA|                null|\n",
      "|              Isbell|         null|         NA|                  NA|                null|\n",
      "|    ABOUDI, SHERAFAT|   06/19/1940|         NA|1019 S Wooster St...|          Discharged|\n",
      "|Above & Beyond, F...|         null|         NA|                  NA|Transition TO CCTALW|\n",
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna({'ADDR':'NA', 'SSN' : 'NA'})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "|             Patient|Date of Birth|        SSN|                ADDR|        Chart_Status|\n",
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "|     AASGAARD, LINDA|   05/14/1943|         NA|1400 CIRCLE CITY ...|            Admitted|\n",
      "|      ABBAS, ALIDAEE|   04/03/1952|         NA|13881 DAWSON ST G...|            Admitted|\n",
      "|    ABDOLI, MOHAMMAD|   08/25/1948|443-74-4056|466 FLAGSHIP RD N...|           Non-admit|\n",
      "|         ABLE, SHARI|   06/06/1939|  566488725|225 North Crescen...|            Admitted|\n",
      "|Above & Beyond, F...|         null|         NA|                  NA|Transition TO CCTALW|\n",
      "|   ABRAHAM, FLORENCE|   01/13/1937|554-58-4615|225 NORTH CRESCEN...|            Admitted|\n",
      "|ABRAMS, JEAN MARI...|   10/31/1954|         NA|9925 LA ALAMEDA A...|            Admitted|\n",
      "|     ABRAMSON, ALIZA|   06/23/1933|         NA|1250 BOYNTON ST G...|            Admitted|\n",
      "|    ABRIOL, THEODORE|   02/14/1965|         NA|25533 RUA MICHELL...|            Admitted|\n",
      "|        ACEBES, NORA|   12/09/1932|         NA|12229 CHANDLER BL...|            Admitted|\n",
      "|  ACEBO, LEONORA (I)|   12/17/1930|613-12-4094|654 S ANZA ST EL ...|           Non-admit|\n",
      "|       ACEVES, LAURA|   03/07/1994|         NA|3312 COFFEE RD MO...|            Admitted|\n",
      "|   ACHA, ARGELIA (I)|   04/09/1924|552-02-9800|22520 MAPLE AVE T...|           Non-admit|\n",
      "|      ACOSTA, ALFRED|   05/07/1930|         NA|9925 LA ALAMEDA A...|            Admitted|\n",
      "|       ACOSTA, LINDA|   12/23/1961|         NA|3175 E. WASHINGTO...|            Admitted|\n",
      "|    ACOSTA, ROSEMARY|   05/13/1953|546-94-2154|1665 M ST FRESNO,...|           Non-admit|\n",
      "|       ACUNA, CARMEN|   07/21/1940|         NA|845 BRDWY #313 CH...|           Non-admit|\n",
      "|       ACUNA, CARMEN|   07/21/1940|         NA| 3223 DUKE ST.  S...|            Admitted|\n",
      "|     ADAME, CLEMENTE|   12/07/1938|466-58-9049|2211 WEST 6TH STR...|            Admitted|\n",
      "|      ADAMES, ELIJAH|   01/18/2018|         NA|13204 MIRA SOL DR...|            Admitted|\n",
      "+--------------------+-------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.filter(df.Chart_Status!='Discharged')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Date of Birth|\n",
      "+-------------+\n",
      "|   01/11/1973|\n",
      "|   12/04/1939|\n",
      "|   10/03/1922|\n",
      "|   03/06/1988|\n",
      "|   05/30/1976|\n",
      "|   10/03/1993|\n",
      "|   11/11/1984|\n",
      "|   08/12/1946|\n",
      "|   03/23/1954|\n",
      "|   05/31/1981|\n",
      "|   04/10/1948|\n",
      "|   05/25/1924|\n",
      "|   05/25/1924|\n",
      "|   05/25/1924|\n",
      "|   05/25/1924|\n",
      "|   04/18/1946|\n",
      "|   01/03/1936|\n",
      "|   05/19/1946|\n",
      "|   12/29/1950|\n",
      "|   04/10/1966|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df.join(df.select('Patient','SSN','ADDR'),(df.Patient==df.Patient) & (df.SSN==df.SSN),how='fullouter')\n",
    "df1.select('Date of Birth').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col)\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    acos(col)\n",
      "        :return: inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    add_months(start, months)\n",
      "        Returns the date that is `months` months after `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 5, 8))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    approxCountDistinct(col, rsd=None)\n",
      "        .. note:: Deprecated in 2.1, use :func:`approx_count_distinct` instead.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    approx_count_distinct(col, rsd=None)\n",
      "        Aggregate function: returns a new :class:`Column` for approximate distinct count of column `col`.\n",
      "        \n",
      "        :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more\n",
      "            efficient to use :func:`countDistinct`\n",
      "        \n",
      "        >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n",
      "        [Row(distinct_ages=2)]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    array(*cols)\n",
      "        Creates a new array column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions that have\n",
      "            the same data type.\n",
      "        \n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    array_contains(col, value)\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        :param col: name of column containing array\n",
      "        :param value: value to check for in array\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asc(col)\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    ascii(col)\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asin(col)\n",
      "        :return: inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    atan(col)\n",
      "        :return: inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    atan2(col1, col2)\n",
      "        :param col1: coordinate on y-axis\n",
      "        :param col2: coordinate on x-axis\n",
      "        :return: the `theta` component of the point\n",
      "           (`r`, `theta`)\n",
      "           in polar coordinates that corresponds to the point\n",
      "           (`x`, `y`) in Cartesian coordinates,\n",
      "           as if computed by `java.lang.Math.atan2()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    avg(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    base64(col)\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bin(col)\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        >>> df.select(bin(df.age).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bitwiseNOT(col)\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    broadcast(df)\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    bround(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    cbrt(col)\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    ceil(col)\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    coalesce(*cols)\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |null|null|\n",
      "        |   1|null|\n",
      "        |null|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          null|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |null|null|             0.0|\n",
      "        |   1|null|             1.0|\n",
      "        |null|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    col(col)\n",
      "        Returns a :class:`Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    collect_list(col)\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_list('age')).collect()\n",
      "        [Row(collect_list(age)=[2, 5, 5])]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    collect_set(col)\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_set('age')).collect()\n",
      "        [Row(collect_set(age)=[5, 2])]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    column(col)\n",
      "        Returns a :class:`Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    concat(*cols)\n",
      "        Concatenates multiple input columns together into a single column.\n",
      "        If all inputs are binary, concat returns an output as binary. Otherwise, it returns as string.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat(df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd123')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    concat_ws(sep, *cols)\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    conv(col, fromBase, toBase)\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    corr(col1, col2)\n",
      "        Returns a new :class:`Column` for the Pearson Correlation Coefficient for ``col1`` and ``col2``.\n",
      "        \n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    cos(col)\n",
      "        :param col: angle in radians\n",
      "        :return: cosine of the angle, as if computed by `java.lang.Math.cos()`.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    cosh(col)\n",
      "        :param col: hyperbolic angle\n",
      "        :return: hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    count(col)\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    countDistinct(col, *cols)\n",
      "        Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    covar_pop(col1, col2)\n",
      "        Returns a new :class:`Column` for the population covariance of ``col1`` and ``col2``.\n",
      "        \n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    covar_samp(col1, col2)\n",
      "        Returns a new :class:`Column` for the sample covariance of ``col1`` and ``col2``.\n",
      "        \n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    crc32(col)\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    create_map(*cols)\n",
      "        Creates a new map column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions that are\n",
      "            grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    cume_dist()\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    current_date()\n",
      "        Returns the current date as a :class:`DateType` column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    current_timestamp()\n",
      "        Returns the current timestamp as a :class:`TimestampType` column.\n",
      "    \n",
      "    date_add(start, days)\n",
      "        Returns the date that is `days` days after `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_format(date, format)\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of the Java class `java.text.SimpleDateFormat` can be used.\n",
      "        \n",
      "        .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n",
      "            specialized implementation.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_sub(start, days)\n",
      "        Returns the date that is `days` days before `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_trunc(format, timestamp)\n",
      "        Returns timestamp truncated to the unit specified by the format.\n",
      "        \n",
      "        :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n",
      "            'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "        >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "        [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "        >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "        [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    datediff(end, start)\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    dayofmonth(col)\n",
      "        Extract the day of the month of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofmonth('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    dayofweek(col)\n",
      "        Extract the day of the week of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofweek('dt').alias('day')).collect()\n",
      "        [Row(day=4)]\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    dayofyear(col)\n",
      "        Extract the day of the year of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofyear('dt').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    decode(col, charset)\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    degrees(col)\n",
      "        Converts an angle measured in radians to an approximately equivalent angle\n",
      "        measured in degrees.\n",
      "        :param col: angle in radians\n",
      "        :return: angle in degrees, as if computed by `java.lang.Math.toDegrees()`\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    dense_rank()\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    desc(col)\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    encode(col, charset)\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    exp(col)\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    explode(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    explode_outer(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Unlike explode, if the array/map is null or empty then null is produced.\n",
      "        \n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+-----+\n",
      "        | id|  an_array| key|value|\n",
      "        +---+----------+----+-----+\n",
      "        |  1|[foo, bar]|   x|  1.0|\n",
      "        |  2|        []|null| null|\n",
      "        |  3|      null|null| null|\n",
      "        +---+----------+----+-----+\n",
      "        \n",
      "        >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+\n",
      "        | id|     a_map| col|\n",
      "        +---+----------+----+\n",
      "        |  1|[x -> 1.0]| foo|\n",
      "        |  1|[x -> 1.0]| bar|\n",
      "        |  2|        []|null|\n",
      "        |  3|      null|null|\n",
      "        +---+----------+----+\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    expm1(col)\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    expr(str)\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        >>> df.select(expr(\"length(name)\")).collect()\n",
      "        [Row(length(name)=5), Row(length(name)=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    factorial(col)\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    first(col, ignorenulls=False)\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    floor(col)\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    format_number(col, d)\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        :param col: the column name of the numeric value to be formatted\n",
      "        :param d: the N decimal places\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    format_string(format, *cols)\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        :param col: the column name of the numeric value to be formatted\n",
      "        :param d: the N decimal places\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    from_json(col, schema, options={})\n",
      "        Parses a column containing a JSON string into a :class:`StructType` or :class:`ArrayType`\n",
      "        of :class:`StructType`\\s with the specified schema. Returns `null`, in the case of an\n",
      "        unparseable string.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param schema: a StructType or ArrayType of StructType to use when parsing the json column.\n",
      "        :param options: options to control parsing. accepts the same options as the json datasource\n",
      "        \n",
      "        .. note:: Since Spark 2.3, the DDL-formatted string or a JSON format string is also\n",
      "                  supported for ``schema``.\n",
      "        \n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
      "        >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n",
      "        [Row(ts='2015-04-08 00:00:00')]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    from_utc_timestamp(timestamp, tz)\n",
      "        Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders\n",
      "        that time as a timestamp in the given time zone. For example, 'GMT+1' would yield\n",
      "        '2017-07-14 03:40:00.0'.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(from_utc_timestamp(df.t, \"PST\").alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    get_json_object(col, path)\n",
      "        Extracts json object from a json string based on json path specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param path: path to the json object to extract\n",
      "        \n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    greatest(*cols)\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    grouping(col)\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | null|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    grouping_id(*cols)\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. note:: The list of columns should match with grouping columns exactly, or empty (means all\n",
      "            the grouping columns).\n",
      "        \n",
      "        >>> df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+-------------+--------+\n",
      "        | name|grouping_id()|sum(age)|\n",
      "        +-----+-------------+--------+\n",
      "        | null|            1|       7|\n",
      "        |Alice|            0|       2|\n",
      "        |  Bob|            0|       5|\n",
      "        +-----+-------------+--------+\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    hash(*cols)\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n",
      "        [Row(hash=-757602832)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    hex(col)\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    hour(col)\n",
      "        Extract the hours of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(hour('ts').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    hypot(col1, col2)\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    initcap(col)\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    input_file_name()\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    instr(str, substr)\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "            could not be found in str.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    isnan(col)\n",
      "        An expression that returns true iff the column is NaN.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    isnull(col)\n",
      "        An expression that returns true iff the column is null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    json_tuple(col, *fields)\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param fields: list of fields to extract\n",
      "        \n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    kurtosis(col)\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    lag(col, count=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `defaultValue` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        :param count: number of row to extend\n",
      "        :param default: default value\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    last(col, ignorenulls=False)\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    last_day(date)\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lead(col, count=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `defaultValue` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        :param count: number of row to extend\n",
      "        :param default: default value\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    least(*cols)\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    length(col)\n",
      "        Computes the character length of string data or number of bytes of binary data.\n",
      "        The length of character data includes the trailing spaces. The length of binary data\n",
      "        includes binary zeros.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    levenshtein(left, right)\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lit(col)\n",
      "        Creates a :class:`Column` of literal value.\n",
      "        \n",
      "        >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n",
      "        [Row(height=5, spark_user=True)]\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    locate(substr, str, pos=1)\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "            could not be found in str.\n",
      "        \n",
      "        :param substr: a string\n",
      "        :param str: a Column of :class:`pyspark.sql.types.StringType`\n",
      "        :param pos: start position (zero based)\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    log(arg1, arg2=None)\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n",
      "        ['0.30102', '0.69897']\n",
      "        \n",
      "        >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n",
      "        ['0.69314', '1.60943']\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    log10(col)\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log1p(col)\n",
      "        Computes the natural logarithm of the given value plus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log2(col)\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        >>> spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect()\n",
      "        [Row(log2=2.0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lower(col)\n",
      "        Converts a string column to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lpad(col, len, pad)\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    ltrim(col)\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    map_keys(col)\n",
      "        Collection function: Returns an unordered array containing the keys of the map.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> from pyspark.sql.functions import map_keys\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\n",
      "        +------+\n",
      "        |  keys|\n",
      "        +------+\n",
      "        |[1, 2]|\n",
      "        +------+\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    map_values(col)\n",
      "        Collection function: Returns an unordered array containing the values of the map.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> from pyspark.sql.functions import map_values\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_values(\"data\").alias(\"values\")).show()\n",
      "        +------+\n",
      "        |values|\n",
      "        +------+\n",
      "        |[a, b]|\n",
      "        +------+\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    max(col)\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    md5(col)\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    mean(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    min(col)\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    minute(col)\n",
      "        Extract the minutes of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(minute('ts').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    monotonically_increasing_id()\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n",
      "        >>> df0.select(monotonically_increasing_id().alias('id')).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    month(col)\n",
      "         Extract the month of a given date as integer.\n",
      "        \n",
      "         >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "         >>> df.select(month('dt').alias('month')).collect()\n",
      "         [Row(month=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    months_between(date1, date2)\n",
      "        Returns the number of months between date1 and date2.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "        >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "        [Row(months=3.9495967...)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    nanvl(col1, col2)\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    next_day(date, dayOfWeek)\n",
      "        Returns the first date which is later than the value of the date column.\n",
      "        \n",
      "        Day of the week parameter is case insensitive, and accepts:\n",
      "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    ntile(n)\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        :param n: an integer\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    pandas_udf(f=None, returnType=None, functionType=None)\n",
      "        Creates a vectorized user defined function (UDF).\n",
      "        \n",
      "        :param f: user-defined function. A python function if used as a standalone function\n",
      "        :param returnType: the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\n",
      "                             Default: SCALAR.\n",
      "        \n",
      "        .. note:: Experimental\n",
      "        \n",
      "        The function type of the UDF can be one of the following:\n",
      "        \n",
      "        1. SCALAR\n",
      "        \n",
      "           A scalar UDF defines a transformation: One or more `pandas.Series` -> A `pandas.Series`.\n",
      "           The returnType should be a primitive data type, e.g., `DoubleType()`.\n",
      "           The length of the returned `pandas.Series` must be of the same as the input `pandas.Series`.\n",
      "        \n",
      "           Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and\n",
      "           :meth:`pyspark.sql.DataFrame.select`.\n",
      "        \n",
      "           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "           >>> from pyspark.sql.types import IntegerType, StringType\n",
      "           >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\n",
      "           >>> @pandas_udf(StringType())  # doctest: +SKIP\n",
      "           ... def to_upper(s):\n",
      "           ...     return s.str.upper()\n",
      "           ...\n",
      "           >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n",
      "           ... def add_one(x):\n",
      "           ...     return x + 1\n",
      "           ...\n",
      "           >>> df = spark.createDataFrame([(1, \"John Doe\", 21)],\n",
      "           ...                            (\"id\", \"name\", \"age\"))  # doctest: +SKIP\n",
      "           >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\n",
      "           ...     .show()  # doctest: +SKIP\n",
      "           +----------+--------------+------------+\n",
      "           |slen(name)|to_upper(name)|add_one(age)|\n",
      "           +----------+--------------+------------+\n",
      "           |         8|      JOHN DOE|          22|\n",
      "           +----------+--------------+------------+\n",
      "        \n",
      "           .. note:: The length of `pandas.Series` within a scalar UDF is not that of the whole input\n",
      "               column, but is the length of an internal batch used for each call to the function.\n",
      "               Therefore, this can be used, for example, to ensure the length of each returned\n",
      "               `pandas.Series`, and can not be used as the column length.\n",
      "        \n",
      "        2. GROUPED_MAP\n",
      "        \n",
      "           A grouped map UDF defines transformation: A `pandas.DataFrame` -> A `pandas.DataFrame`\n",
      "           The returnType should be a :class:`StructType` describing the schema of the returned\n",
      "           `pandas.DataFrame`.\n",
      "           The length of the returned `pandas.DataFrame` can be arbitrary and the columns must be\n",
      "           indexed so that their position matches the corresponding field in the schema.\n",
      "        \n",
      "           Grouped map UDFs are used with :meth:`pyspark.sql.GroupedData.apply`.\n",
      "        \n",
      "           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "           >>> df = spark.createDataFrame(\n",
      "           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "           ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      "           >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
      "           ... def normalize(pdf):\n",
      "           ...     v = pdf.v\n",
      "           ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      "           >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\n",
      "           +---+-------------------+\n",
      "           | id|                  v|\n",
      "           +---+-------------------+\n",
      "           |  1|-0.7071067811865475|\n",
      "           |  1| 0.7071067811865475|\n",
      "           |  2|-0.8320502943378437|\n",
      "           |  2|-0.2773500981126146|\n",
      "           |  2| 1.1094003924504583|\n",
      "           +---+-------------------+\n",
      "        \n",
      "           .. note:: If returning a new `pandas.DataFrame` constructed with a dictionary, it is\n",
      "               recommended to explicitly index the columns by name to ensure the positions are correct,\n",
      "               or alternatively use an `OrderedDict`.\n",
      "               For example, `pd.DataFrame({'id': ids, 'a': data}, columns=['id', 'a'])` or\n",
      "               `pd.DataFrame(OrderedDict([('id', ids), ('a', data)]))`.\n",
      "        \n",
      "           .. seealso:: :meth:`pyspark.sql.GroupedData.apply`\n",
      "        \n",
      "        .. note:: The user-defined functions are considered deterministic by default. Due to\n",
      "            optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "            more times than it is present in the query. If your function is not deterministic, call\n",
      "            `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> @pandas_udf('double', PandasUDFType.SCALAR)  # doctest: +SKIP\n",
      "        ... def random(v):\n",
      "        ...     import numpy as np\n",
      "        ...     import pandas as pd\n",
      "        ...     return pd.Series(np.random.randn(len(v))\n",
      "        >>> random = random.asNondeterministic()  # doctest: +SKIP\n",
      "        \n",
      "        .. note:: The user-defined functions do not support conditional expressions or short circuiting\n",
      "            in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "            can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        .. note:: The user-defined functions do not take keyword arguments on the calling side.\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    percent_rank()\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    posexplode(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(posexplode(eDF.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> eDF.select(posexplode(eDF.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    posexplode_outer(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\n",
      "        \n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+----+-----+\n",
      "        | id|  an_array| pos| key|value|\n",
      "        +---+----------+----+----+-----+\n",
      "        |  1|[foo, bar]|   0|   x|  1.0|\n",
      "        |  2|        []|null|null| null|\n",
      "        |  3|      null|null|null| null|\n",
      "        +---+----------+----+----+-----+\n",
      "        >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+----+\n",
      "        | id|     a_map| pos| col|\n",
      "        +---+----------+----+----+\n",
      "        |  1|[x -> 1.0]|   0| foo|\n",
      "        |  1|[x -> 1.0]|   1| bar|\n",
      "        |  2|        []|null|null|\n",
      "        |  3|      null|null|null|\n",
      "        +---+----------+----+----+\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    pow(col1, col2)\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    quarter(col)\n",
      "        Extract the quarter of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(quarter('dt').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    radians(col)\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle\n",
      "        measured in radians.\n",
      "        :param col: angle in degrees\n",
      "        :return: angle in radians, as if computed by `java.lang.Math.toRadians()`\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    rand(seed=None)\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        from U[0.0, 1.0].\n",
      "        \n",
      "        >>> df.withColumn('rand', rand(seed=42) * 3).collect()\n",
      "        [Row(age=2, name='Alice', rand=1.1568609015300986),\n",
      "         Row(age=5, name='Bob', rand=1.403379671529166)]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    randn(seed=None)\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        >>> df.withColumn('randn', randn(seed=42)).collect()\n",
      "        [Row(age=2, name='Alice', randn=-0.7556247885860078),\n",
      "        Row(age=5, name='Bob', randn=-0.0861619008451133)]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    rank()\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    regexp_extract(str, pattern, idx)\n",
      "        Extract a specific group matched by a Java regex, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    regexp_replace(str, pattern, replacement)\n",
      "        Replace all substrings of the specified string value that match regexp with rep.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_replace('str', '(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    repeat(col, n)\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    reverse(col)\n",
      "        Reverses the string column and returns it as a new string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    rint(col)\n",
      "        Returns the double value that is closest in value to the argument and is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    round(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    row_number()\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    rpad(col, len, pad)\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    rtrim(col)\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    second(col)\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(second('ts').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sha1(col)\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sha2(col, numBits)\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        >>> digests = df.select(sha2(df.name, 256).alias('s')).collect()\n",
      "        >>> digests[0]\n",
      "        Row(s='3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043')\n",
      "        >>> digests[1]\n",
      "        Row(s='cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961')\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftLeft(col, numBits)\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftRight(col, numBits)\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftRightUnsigned(col, numBits)\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftRightUnsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    signum(col)\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sin(col)\n",
      "        :param col: angle in radians\n",
      "        :return: sine of the angle, as if computed by `java.lang.Math.sin()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sinh(col)\n",
      "        :param col: hyperbolic angle\n",
      "        :return: hyperbolic sine of the given value,\n",
      "                 as if computed by `java.lang.Math.sinh()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    size(col)\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    skewness(col)\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    sort_array(col, asc=True)\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1]), Row(r=[1]), Row(r=[])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    soundex(col)\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    spark_partition_id()\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. note:: This is indeterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    split(str, pattern)\n",
      "        Splits str around pattern (pattern is a regular expression).\n",
      "        \n",
      "        .. note:: pattern is a string represent the regular expression.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab12cd',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[0-9]+').alias('s')).collect()\n",
      "        [Row(s=['ab', 'cd'])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sqrt(col)\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    stddev(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_pop(col)\n",
      "        Aggregate function: returns population standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_samp(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    struct(*cols)\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions\n",
      "        \n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    substring(str, pos, len)\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    substring_index(str, delim, count)\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sum(col)\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    sumDistinct(col)\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    tan(col)\n",
      "        :param col: angle in radians\n",
      "        :return: tangent of the given value, as if computed by `java.lang.Math.tan()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    tanh(col)\n",
      "        :param col: hyperbolic angle\n",
      "        :return: hyperbolic tangent of the given value,\n",
      "                 as if computed by `java.lang.Math.tanh()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toDegrees(col)\n",
      "        .. note:: Deprecated in 2.1, use :func:`degrees` instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toRadians(col)\n",
      "        .. note:: Deprecated in 2.1, use :func:`radians` instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    to_date(col, format=None)\n",
      "        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to\n",
      "        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "        is omitted (equivalent to ``col.cast(\"date\")``).\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        .. versionadded:: 2.2\n",
      "    \n",
      "    to_json(col, options={})\n",
      "        Converts a column containing a :class:`StructType`, :class:`ArrayType` of\n",
      "        :class:`StructType`\\s, a :class:`MapType` or :class:`ArrayType` of :class:`MapType`\\s\n",
      "        into a JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        :param col: name of column containing the struct, array of the structs, the map or\n",
      "            array of the maps.\n",
      "        :param options: options to control converting. accepts the same options as the json datasource\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(name='Alice', age=2))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(name='Alice', age=2), Row(name='Bob', age=3)])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, {\"name\": \"Alice\"})]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    to_timestamp(col, format=None)\n",
      "        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to\n",
      "        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "        is omitted (equivalent to ``col.cast(\"timestamp\")``).\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        .. versionadded:: 2.2\n",
      "    \n",
      "    to_utc_timestamp(timestamp, tz)\n",
      "        Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time\n",
      "        zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield\n",
      "        '2017-07-14 01:40:00.0'.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['ts'])\n",
      "        >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    translate(srcCol, matching, replace)\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        The translate will happen when any character in the string matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trim(col)\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trunc(date, format)\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        :param format: 'year', 'yyyy', 'yy' or 'month', 'mon', 'mm'\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    udf(f=None, returnType=StringType)\n",
      "        Creates a user defined function (UDF).\n",
      "        \n",
      "        .. note:: The user-defined functions are considered deterministic by default. Due to\n",
      "            optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "            more times than it is present in the query. If your function is not deterministic, call\n",
      "            `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> import random\n",
      "        >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "        \n",
      "        .. note:: The user-defined functions do not support conditional expressions or short circuiting\n",
      "            in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "            can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        .. note:: The user-defined functions do not take keyword arguments on the calling side.\n",
      "        \n",
      "        :param f: python function if used as a standalone function\n",
      "        :param returnType: the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    unbase64(col)\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unhex(col)\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, return null if fail.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "        [Row(unix_time=1428476400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    upper(col)\n",
      "        Converts a string column to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    var_pop(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    var_samp(col)\n",
      "        Aggregate function: returns the unbiased variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    variance(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    weekofyear(col)\n",
      "        Extract the week number of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekofyear(df.dt).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    when(condition, value)\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "        \n",
      "        :param condition: a boolean :class:`Column` expression.\n",
      "        :param value: a literal value, or a :class:`Column` expression.\n",
      "        \n",
      "        >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=4)]\n",
      "        \n",
      "        >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=None)]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    year(col)\n",
      "        Extract the year of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(year('dt').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "\n",
      "DATA\n",
      "    __all__ = ['abs', 'acos', 'add_months', 'approxCountDistinct', 'approx...\n",
      "\n",
      "FILE\n",
      "    c:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\functions.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "help(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        Chart_Status|count|\n",
      "+--------------------+-----+\n",
      "|  PALACIOS, MICHELLE|   10|\n",
      "|           Non-admit| 1284|\n",
      "|       WYNN, THERESA|    1|\n",
      "|           Wait List|    7|\n",
      "|          KIM, NAOMI|   20|\n",
      "|       QUIROZ, DAISY|   33|\n",
      "|           GO, DIANA|   20|\n",
      "|Transition TO CCTALW|   90|\n",
      "|     BERMAN, DEBORAH|   12|\n",
      "|       Raiken, Susan|   22|\n",
      "|       SHAHAM, YAFIT|    7|\n",
      "|            Admitted| 2749|\n",
      "|      BEMEL, MELANIE|    5|\n",
      "|      GEWIRTZ, JENNA|    1|\n",
      "|CCA Preadmit Slot...|    3|\n",
      "|     ROJAS, JENNIFER|   13|\n",
      "|    GUEVARA, NATALIE|    4|\n",
      "|       REYES, MARCUS|    2|\n",
      "|Transition To CCT...|   22|\n",
      "|          NAVAL, ANA|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Chart_Status').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------+\n",
      "|        Chart_Status|CNT|ADDR_CNT|\n",
      "+--------------------+---+--------+\n",
      "|  PALACIOS, MICHELLE|  1|      10|\n",
      "|           Non-admit|803|    1284|\n",
      "|       WYNN, THERESA|  1|       1|\n",
      "|           Wait List|  4|       7|\n",
      "|          KIM, NAOMI|  1|      20|\n",
      "|       QUIROZ, DAISY|  1|      33|\n",
      "|           GO, DIANA|  1|      20|\n",
      "|Transition TO CCTALW| 78|      90|\n",
      "|     BERMAN, DEBORAH|  1|      12|\n",
      "|       Raiken, Susan|  1|      22|\n",
      "|       SHAHAM, YAFIT|  1|       7|\n",
      "|            Admitted|451|    2749|\n",
      "|      BEMEL, MELANIE|  1|       5|\n",
      "|      GEWIRTZ, JENNA|  1|       1|\n",
      "|CCA Preadmit Slot...|  2|       3|\n",
      "|    GUEVARA, NATALIE|  1|       4|\n",
      "|     ROJAS, JENNIFER|  1|      13|\n",
      "|       REYES, MARCUS|  1|       2|\n",
      "|Transition To CCT...| 20|      22|\n",
      "|          NAVAL, ANA|  1|       2|\n",
      "+--------------------+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Chart_Status').agg(F.countDistinct('SSN').alias('CNT'),F.count('ADDR').alias('ADDR_CNT')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c=4560)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg(F.count(df.Chart_Status).alias('c')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+-------------------+\n",
      "|        Chart_Status|grouping(Chart_Status)|count(Chart_Status)|\n",
      "+--------------------+----------------------+-------------------+\n",
      "|                null|                     1|               4560|\n",
      "|     ARIAS, GABRIELA|                     0|                  4|\n",
      "|            Admitted|                     0|               2749|\n",
      "|      BEMEL, MELANIE|                     0|                  5|\n",
      "|     BERMAN, DEBORAH|                     0|                 12|\n",
      "|        CCA Preadmit|                     0|                  1|\n",
      "|CCA Preadmit Appl...|                     0|                  9|\n",
      "|CCA Preadmit Appl...|                     0|                  6|\n",
      "|CCA Preadmit Slot...|                     0|                  3|\n",
      "|   CORTEZ, CELESTINA|                     0|                  2|\n",
      "|      ESCANO, KARINA|                     0|                  2|\n",
      "|      FIGUEROA, ELIA|                     0|                 24|\n",
      "|      GEWIRTZ, JENNA|                     0|                  1|\n",
      "|           GO, DIANA|                     0|                 20|\n",
      "|    GUEVARA, NATALIE|                     0|                  4|\n",
      "|      HARRIS, SOPHIA|                     0|                  2|\n",
      "|      Istrin, Yehuda|                     0|                  1|\n",
      "|          KIM, NAOMI|                     0|                 20|\n",
      "|        LOPEZ, JENNY|                     0|                  3|\n",
      "|       MARKIE, DAVID|                     0|                  1|\n",
      "+--------------------+----------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cube('Chart_Status').agg(F.grouping('Chart_Status'),F.count('Chart_Status')).orderBy('Chart_Status').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|        Chart_Status|count(1)|\n",
      "+--------------------+--------+\n",
      "|  PALACIOS, MICHELLE|      10|\n",
      "|           Non-admit|    1284|\n",
      "|       WYNN, THERESA|       1|\n",
      "|           Wait List|       7|\n",
      "|          KIM, NAOMI|      20|\n",
      "|       QUIROZ, DAISY|      33|\n",
      "|           GO, DIANA|      20|\n",
      "|Transition TO CCTALW|      90|\n",
      "|     BERMAN, DEBORAH|      12|\n",
      "|       Raiken, Susan|      22|\n",
      "|       SHAHAM, YAFIT|       7|\n",
      "|            Admitted|    2749|\n",
      "|      BEMEL, MELANIE|       5|\n",
      "|      GEWIRTZ, JENNA|       1|\n",
      "|CCA Preadmit Slot...|       3|\n",
      "|     ROJAS, JENNIFER|      13|\n",
      "|    GUEVARA, NATALIE|       4|\n",
      "|       REYES, MARCUS|       2|\n",
      "|Transition To CCT...|      22|\n",
      "|          NAVAL, ANA|       2|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "data12=spark.sql('select Chart_Status,count(*) from df group by Chart_Status')\n",
    "data12.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+--------------------+--------------------+---+\n",
      "|             Patient|Date of Birth|        SSN|                ADDR|        Chart_Status| ID|\n",
      "+--------------------+-------------+-----------+--------------------+--------------------+---+\n",
      "|     AASGAARD, LINDA|   05/14/1943|         NA|1400 CIRCLE CITY ...|            Admitted|  1|\n",
      "|      ABBAS, ALIDAEE|   04/03/1952|         NA|13881 DAWSON ST G...|            Admitted|  1|\n",
      "|    ABDOLI, MOHAMMAD|   08/25/1948|443-74-4056|466 FLAGSHIP RD N...|           Non-admit|  1|\n",
      "|         ABLE, SHARI|   06/06/1939|  566488725|225 North Crescen...|            Admitted|  1|\n",
      "|Above & Beyond, F...|         null|         NA|                  NA|Transition TO CCTALW|  1|\n",
      "|   ABRAHAM, FLORENCE|   01/13/1937|554-58-4615|225 NORTH CRESCEN...|            Admitted|  1|\n",
      "|ABRAMS, JEAN MARI...|   10/31/1954|         NA|9925 LA ALAMEDA A...|            Admitted|  1|\n",
      "|     ABRAMSON, ALIZA|   06/23/1933|         NA|1250 BOYNTON ST G...|            Admitted|  1|\n",
      "|    ABRIOL, THEODORE|   02/14/1965|         NA|25533 RUA MICHELL...|            Admitted|  1|\n",
      "|        ACEBES, NORA|   12/09/1932|         NA|12229 CHANDLER BL...|            Admitted|  1|\n",
      "|  ACEBO, LEONORA (I)|   12/17/1930|613-12-4094|654 S ANZA ST EL ...|           Non-admit|  1|\n",
      "|       ACEVES, LAURA|   03/07/1994|         NA|3312 COFFEE RD MO...|            Admitted|  1|\n",
      "|   ACHA, ARGELIA (I)|   04/09/1924|552-02-9800|22520 MAPLE AVE T...|           Non-admit|  1|\n",
      "|      ACOSTA, ALFRED|   05/07/1930|         NA|9925 LA ALAMEDA A...|            Admitted|  1|\n",
      "|       ACOSTA, LINDA|   12/23/1961|         NA|3175 E. WASHINGTO...|            Admitted|  1|\n",
      "|    ACOSTA, ROSEMARY|   05/13/1953|546-94-2154|1665 M ST FRESNO,...|           Non-admit|  1|\n",
      "|       ACUNA, CARMEN|   07/21/1940|         NA|845 BRDWY #313 CH...|           Non-admit|  1|\n",
      "|       ACUNA, CARMEN|   07/21/1940|         NA| 3223 DUKE ST.  S...|            Admitted|  1|\n",
      "|     ADAME, CLEMENTE|   12/07/1938|466-58-9049|2211 WEST 6TH STR...|            Admitted|  1|\n",
      "|      ADAMES, ELIJAH|   01/18/2018|         NA|13204 MIRA SOL DR...|            Admitted|  1|\n",
      "+--------------------+-------------+-----------+--------------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= df.withColumn('ID',F.lit(1))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Patient: string (nullable = true)\n",
      " |-- Date of Birth: string (nullable = true)\n",
      " |-- SSN: string (nullable = false)\n",
      " |-- ADDR: string (nullable = false)\n",
      " |-- Chart_Status: string (nullable = true)\n",
      " |-- ID: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Patient: string (nullable = true)\n",
      " |-- Date of Birth: string (nullable = true)\n",
      " |-- SSN: string (nullable = false)\n",
      " |-- ADDR: string (nullable = false)\n",
      " |-- Chart_Status: string (nullable = true)\n",
      " |-- ID: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df1.withColumn('ID',F.col('ID').cast('string'))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
